{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMuLgh69etuRY+CvofBoFQk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Multinomial Language Model"],"metadata":{"id":"0I8ST4X3R8En"}},{"cell_type":"markdown","source":["This notebook impliments a multinomial distribution language model for information retrieval of documents based on a query search. \n","\n","The documents, from a fixed repository, are scored and ranked for similarity against a test set of queries. The output results are used for evaluation using the trec_eval tool.\n","\n","In the final section, the notebook allows a user to manually enter a free form text search to test this against the existing documents repository, using the same ranking model - useful for exploratory testing."],"metadata":{"id":"wqxL6UOySDcX"}},{"cell_type":"markdown","source":["## Imports and setup"],"metadata":{"id":"D8L-tkAoSF-R"}},{"cell_type":"code","source":["import math\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","import csv\n","import os\n","import nltk\n","from nltk.corpus import reuters\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.text import log\n","import xml.etree.ElementTree as ET\n","\n","nltk.download('reuters')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","stop_words = set(stopwords.words(\"english\"))"],"metadata":{"id":"fjhGVKoI2eJU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678982459840,"user_tz":0,"elapsed":4380,"user":{"displayName":"Anthony Forde","userId":"14193265044366032824"}},"outputId":"2c92a321-6435-434f-b23a-4e907e0ce65c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package reuters to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"FIvBHtii42WD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678982493108,"user_tz":0,"elapsed":23563,"user":{"displayName":"Anthony Forde","userId":"14193265044366032824"}},"outputId":"9c97be5b-4dd7-4f4b-d72a-83ecf93c7b4c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Part 1 - Ranking by document titles\n","In this section we score each search query for document title and create a shortlist of the top 100 relevant documents (by title)."],"metadata":{"id":"K-adKKYGpU6O"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"Kop02SIpozYJ"}},{"cell_type":"code","source":["# Create base dataframe for recording results\n","df_Results = pd.DataFrame(columns=['Query_ID','Doc_ID', 'Multinomial_Score','Query_Desc', 'Doc_Desc'])"],"metadata":{"id":"VhexWxi746vz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_Results.drop(df_Results.index,inplace=True)"],"metadata":{"id":"WeLy0YwT462z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bring in the data"],"metadata":{"id":"WOji2c3z5EST"}},{"cell_type":"markdown","source":["Indexed queries and documents preprepared from previous notebook"],"metadata":{"id":"CEAX5MqCSKpu"}},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/CA6005I - Mechanics of Search/Assignment1/Files_Indexed\")"],"metadata":{"id":"GCBWLIQq5Efa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Document titles file"],"metadata":{"id":"lsbPQdztSN6X"}},{"cell_type":"code","source":["# Import from prepared CSV file - read doc IDs and titles to array\n","with open('Indexed_Titles.csv', 'r') as file:\n","   reader = csv.reader(file)\n","   documents = []\n","   documentIDs = []\n","   for row in reader:\n","        documentIDs.append(row[1])\n","        documents.append(row[2])"],"metadata":{"id":"fARfhdXm4xOU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Search queries file"],"metadata":{"id":"BtMIXs9JSTk4"}},{"cell_type":"code","source":["# Import from prepared CSV file - read query IDs and search strings to array\n","with open('Indexed_Queries.csv', 'r') as file:\n","    reader = csv.reader(file)\n","    queries = []\n","    queryIDs = []\n","    for row in reader:\n","        queries.append(row[2])\n","        queryIDs.append((row[1]))"],"metadata":{"id":"0jmezmKj50M2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing"],"metadata":{"id":"cMNzrSXuShMX"}},{"cell_type":"code","source":["# Tokenize the documents into words\n","tokenized_docs = []\n","for doc in documents:\n","    words = doc.lower().split()\n","    words = [word for word in words if word not in stop_words]\n","    tokenized_docs.append(words)\n","\n","# Compute the vocabulary\n","vocab = set([word for doc in tokenized_docs for word in doc])\n","\n","# Compute the document-term matrix\n","doc_term_matrix = np.zeros((len(documents), len(vocab)))\n","for i, doc in enumerate(tokenized_docs):\n","    for j, word in enumerate(vocab):\n","        doc_term_matrix[i, j] = doc.count(word)"],"metadata":{"id":"LUHKo3JI5TRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocessed_docs = []\n","for doc in documents:\n","    words = doc.lower().split()\n","    words = [word for word in words if word not in stop_words]\n","    preprocessed_docs.append(words)"],"metadata":{"id":"Zn4FAUzO5OaB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Process queries and scores"],"metadata":{"id":"rGhPsOElS2iu"}},{"cell_type":"markdown","source":["For each query, a similarity score is computed for every document"],"metadata":{"id":"1gKOIXGt6uF7"}},{"cell_type":"code","source":["current_query = 0\n","# For each query\n","for query in queries:\n","\n","  doc_scores = []\n","\n","  rawquery = queries[current_query]\n","  queryID = queryIDs[current_query]\n","\n","  tokenized_query = query.lower().split()\n","  tokenized_query = [string for string in tokenized_query if string not in stop_words]  \n","\n","  # Compute the query-term vector\n","  query_term_vector = np.zeros(len(vocab))\n","  for i, word in enumerate(vocab):\n","      query_term_vector[i] = tokenized_query.count(word)\n","\n","  # Compute the document scores\n","  doc_scores = np.dot(doc_term_matrix, query_term_vector)\n","\n","  current_score = 0\n","  # For each computed similarity score\n","  for score in doc_scores:\n","    #print(\"Query: \" + str(current_query) + \" Score: \" + str(current_score) + \" \" + str(score))\n","    # Append a new row to the results dataframe\n","    new_row = [int(queryID), int(documentIDs[current_score]), score, rawquery, documents[current_score]]\n","    df_Results = df_Results.append(pd.Series(new_row, index=df_Results.columns), ignore_index=True)\n","    current_score += 1  \n","  \n","  current_query += 1"],"metadata":{"id":"7CqdBREaLIY2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sort the results: group by query ID, then sorted by scores ascending for each query. Finally, optionally, retain only top results for each query search, e.g. 10, 50, 100..."],"metadata":{"id":"lcovoVUiRc1w"}},{"cell_type":"code","source":["df_SortedResults = df_Results.sort_values(by=['Query_ID', 'Multinomial_Score'], ascending=[True, False])"],"metadata":{"id":"2FGzOIvxRVxv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Restrict to top 100 results\n","df_TopResults = df_SortedResults.groupby('Query_ID').head(100).reset_index(drop=True)"],"metadata":{"id":"WboKCIwDRV3u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults.insert(4, 'Rank',0)"],"metadata":{"id":"L8QIPCDXjSdv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults['Rank'] = df_TopResults.groupby('Query_ID').cumcount() + 1"],"metadata":{"id":"2DQ5qqOBlN1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Export final results to CSV for final analysis (outside of this notebook)\n","df_TopResults.to_csv(\"Export_Multinomial_Top100_by_Title.csv\")"],"metadata":{"id":"BNijhebvH-lx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 2 - Ranking by document contents\n","In this section we score each search query for document contents (main body of the document) and create a shortlist of the top 100 relevant documents (by contents)."],"metadata":{"id":"zofD2K6Yphw1"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"rvJxrW_Zphw2"}},{"cell_type":"code","source":["# Create base dataframe for recording results\n","df_Results = pd.DataFrame(columns=['Query_ID','Doc_ID', 'Multinomial_Score'])"],"metadata":{"id":"w8yk2Vg5phw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_Results.drop(df_Results.index,inplace=True)"],"metadata":{"id":"TUBpjft5phw2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bring in the data"],"metadata":{"id":"m-VsliFfphw2"}},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/CA6005I - Mechanics of Search/Assignment1/Files_Indexed\")"],"metadata":{"id":"345Hfyzhphw2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Indexed queries and documents preprepared from previous notebook"],"metadata":{"id":"0P2j7EIwphw3"}},{"cell_type":"markdown","source":["Document contents file"],"metadata":{"id":"rkRyE4G5phw3"}},{"cell_type":"code","source":["# Import from prepared CSV file - read doc IDs and contents to array\n","with open('Indexed_Contents.csv', 'r') as file:\n","   reader = csv.reader(file)\n","   documents = []\n","   documentIDs = []\n","   for row in reader:\n","        documentIDs.append(row[1])\n","        documents.append(row[2])"],"metadata":{"id":"K4evlRBgphw3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Search queries file"],"metadata":{"id":"ipPj5Lwwphw3"}},{"cell_type":"code","source":["# Import from prepared CSV file - read query IDs and search strings to array\n","with open('Indexed_Queries.csv', 'r') as file:\n","    reader = csv.reader(file)\n","    queries = []\n","    queryIDs = []\n","    for row in reader:\n","        queries.append(row[2])\n","        queryIDs.append((row[1]))"],"metadata":{"id":"-9ujuP4Sphw3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing"],"metadata":{"id":"dCCciOoiphw3"}},{"cell_type":"code","source":["# Tokenize the documents into words\n","tokenized_docs = []\n","for doc in documents:\n","    words = doc.lower().split()\n","    words = [word for word in words if word not in stop_words]\n","    tokenized_docs.append(words)\n","\n","# Compute the vocabulary\n","vocab = set([word for doc in tokenized_docs for word in doc])\n","\n","# Compute the document-term matrix\n","doc_term_matrix = np.zeros((len(documents), len(vocab)))\n","for i, doc in enumerate(tokenized_docs):\n","    for j, word in enumerate(vocab):\n","        doc_term_matrix[i, j] = doc.count(word)"],"metadata":{"id":"GKXoiK0yphw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocessed_docs = []\n","for doc in documents:\n","    words = doc.lower().split()\n","    words = [word for word in words if word not in stop_words]\n","    preprocessed_docs.append(words)"],"metadata":{"id":"KyIHGOLVphw4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Similarity scoring"],"metadata":{"id":"zbHXDzqYphw4"}},{"cell_type":"markdown","source":["For each query, a similarity score is computed for every document"],"metadata":{"id":"LDL0Okkiphw4"}},{"cell_type":"code","source":["current_query = 0\n","# For each query\n","for query in queries:\n","\n","  doc_scores = []\n","\n","  rawquery = queries[current_query]\n","  queryID = queryIDs[current_query]\n","\n","  tokenized_query = query.lower().split()\n","  tokenized_query = [string for string in tokenized_query if string not in stop_words]  \n","\n","  # Compute the query-term vector\n","  query_term_vector = np.zeros(len(vocab))\n","  for i, word in enumerate(vocab):\n","      query_term_vector[i] = tokenized_query.count(word)\n","\n","  # Compute the document scores\n","  doc_scores = np.dot(doc_term_matrix, query_term_vector)\n","\n","  current_score = 0\n","  # For each computed similarity score\n","  for score in doc_scores:\n","    #print(\"Query: \" + str(current_query) + \" Score: \" + str(current_score) + \" \" + str(score))\n","    # Append a new row to the results dataframe\n","    new_row = [int(queryID), int(documentIDs[current_score]), score]\n","    df_Results = df_Results.append(pd.Series(new_row, index=df_Results.columns), ignore_index=True)\n","    current_score += 1  \n","  \n","  current_query += 1"],"metadata":{"id":"a-eoXpGOphw4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sort the results: group by query ID, then sorted by scores ascending for each query. Finally, optionally, retain only top results for each query search, e.g. 10, 50, 100..."],"metadata":{"id":"8lX3VBPGphw5"}},{"cell_type":"code","source":["df_TopResults"],"metadata":{"id":"ZJ8FOioZw0fK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_SortedResults = df_Results.sort_values(by=['Query_ID', 'Multinomial_Score'], ascending=[True, False])"],"metadata":{"id":"KbI8CCzWphw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Restrict to top X results\n","df_TopResults = df_SortedResults.groupby('Query_ID').head(100).reset_index(drop=True)"],"metadata":{"id":"9XxK9U0qphw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults['Rank'] = df_TopResults.groupby('Query_ID').cumcount() + 1"],"metadata":{"id":"zF2kLmLophw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults.to_csv(\"Export_Multinomial_Top100_Queries_by_Content.csv\")"],"metadata":{"id":"RDGaZiKFphw6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 3 - Test a single query\n","\n","Enter a freeform query search against the documents repository."],"metadata":{"id":"feo7zz5V1pLX"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"RWUh01Lp1pLe"}},{"cell_type":"markdown","source":["Read indexed document titles data into dataframe - title to be used in search results summary\n"],"metadata":{"id":"8GxDLs6JBxrn"}},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/CA6005I - Mechanics of Search/Assignment1/Files_Indexed\")\n","df_titles = []\n","df_titles = pd.DataFrame(columns=['Index','Doc_ID', 'Title'])\n","title_data = pd.read_csv(\"Indexed_Titles.csv\", names=['Index','Doc_ID', 'Title'])\n","df_titles = df_titles.append(title_data, ignore_index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7PIkssK06Gim","executionInfo":{"status":"ok","timestamp":1678982559061,"user_tz":0,"elapsed":1704,"user":{"displayName":"Anthony Forde","userId":"14193265044366032824"}},"outputId":"e77bad4c-4d54-4c17-c14c-493e370cdbaa"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-c9fbfc68a0e5>:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df_titles = df_titles.append(title_data, ignore_index=True)\n"]}]},{"cell_type":"markdown","source":["Create base dataframe for recording results\n"],"metadata":{"id":"4PAfITjmB35c"}},{"cell_type":"code","source":["df_Results =[]\n","df_Results = pd.DataFrame(columns=['Query_ID','Doc_ID', 'LM_Score', 'Rank', 'Title'])\n","df_Results.drop(df_Results.index,inplace=True)"],"metadata":{"id":"MI63nVrS1pLe","executionInfo":{"status":"ok","timestamp":1678982906944,"user_tz":0,"elapsed":179,"user":{"displayName":"Anthony Forde","userId":"14193265044366032824"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Bring in the data"],"metadata":{"id":"IE-6bP0o1pLf"}},{"cell_type":"markdown","source":["Indexed queries and documents preprepared from previous notebook"],"metadata":{"id":"kL9xmPwc1pLf"}},{"cell_type":"markdown","source":["Document contents file"],"metadata":{"id":"ggGXIU1U1pLf"}},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/CA6005I - Mechanics of Search/Assignment1/Files_Indexed\")\n","\n","# Import from prepared CSV file - read doc IDs and contents to array\n","with open('Indexed_Contents.csv', 'r') as file:\n","   reader = csv.reader(file)\n","   documents = []\n","   documentIDs = []\n","   for row in reader:\n","        documentIDs.append(row[1])\n","        documents.append(row[2])"],"metadata":{"id":"1EZ2KBWP1pLg","executionInfo":{"status":"ok","timestamp":1678982669940,"user_tz":0,"elapsed":602,"user":{"displayName":"Anthony Forde","userId":"14193265044366032824"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing"],"metadata":{"id":"jUj0XnR81pLg"}},{"cell_type":"code","source":["# Tokenize the documents into words\n","tokenized_docs = []\n","for doc in documents:\n","    words = doc.lower().split()\n","    words = [word for word in words if word not in stop_words]\n","    tokenized_docs.append(words)\n","\n","# Compute the vocabulary\n","vocab = set([word for doc in tokenized_docs for word in doc])\n","\n","# Compute the document-term matrix\n","doc_term_matrix = np.zeros((len(documents), len(vocab)))\n","for i, doc in enumerate(tokenized_docs):\n","    for j, word in enumerate(vocab):\n","        doc_term_matrix[i, j] = doc.count(word)"],"metadata":{"id":"bR_tdKBu1pLg","executionInfo":{"status":"ok","timestamp":1678982719861,"user_tz":0,"elapsed":37526,"user":{"displayName":"Anthony Forde","userId":"14193265044366032824"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Process queries\n","- Type a query ==> similarity score is computed for every document.\n","\n","- Results display top 10 ranked documents and a title summary for each.\n","\n","- Open a document file using the listed document ID."],"metadata":{"id":"XmKgClDm1pLh"}},{"cell_type":"markdown","source":["Enter query"],"metadata":{"id":"klgpdPBy1pLh"}},{"cell_type":"code","source":["#query = \"what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft\"\n","#query = \"what is the captial of France?\"\n","query = \"inviscid hypersonic airflows with coupled\"\n","\n","\n","doc_scores = []\n","df_Results.drop(df_Results.index,inplace=True)\n","\n","tokenized_query = query.lower().split()\n","tokenized_query = [string for string in tokenized_query if string not in stop_words]  \n","\n","# Compute the query-term vector\n","query_term_vector = np.zeros(len(vocab))\n","for i, word in enumerate(vocab):\n","    query_term_vector[i] = tokenized_query.count(word)\n","\n","# Compute the document scores\n","doc_scores = np.dot(doc_term_matrix, query_term_vector)\n","\n","current_score = 0\n","# For each computed similarity score\n","for score in doc_scores:\n","  # print(\"Query: \" + str(current_query) + \" Score: \" + str(current_score) + \" \" + str(score))\n","  # Append a new row to the results dataframe\n","  new_row = [\"USER\", int(documentIDs[current_score]), score, 0, \"\"]\n","  df_Results = df_Results.append(pd.Series(new_row, index=df_Results.columns), ignore_index=True)\n","  current_score += 1  "],"metadata":{"id":"074kCBaL1pLh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sort the results: group by query ID, then sorted by scores ascending for each query. Finally, optionally, retain only top results for each query search, e.g. 10, 50, 100..."],"metadata":{"id":"9JmsX4L-1pLh"}},{"cell_type":"code","source":["df_SortedResults = []\n","df_TopResults = []\n","df_SortedResults = df_Results.sort_values(by=['Query_ID', 'LM_Score'], ascending=[True, False])\n","# Restrict to top 10 results\n","df_TopResults = df_SortedResults.groupby('Query_ID').head(10).reset_index(drop=True)\n","df_TopResults['Rank'] = df_TopResults.groupby('Query_ID').cumcount() + 1\n","\n","for index, row in df_titles.iterrows():\n","  df_TopResults.loc[(df_TopResults.Doc_ID == row['Doc_ID']), 'Title'] = row['Title']\n","\n","\n","print(\"--- QUERY: \" + query + \"\\n\")\n","df_TopResults"],"metadata":{"id":"RFJ0c2Mi3lU1","colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"status":"ok","timestamp":1678983240496,"user_tz":0,"elapsed":767,"user":{"displayName":"Anthony Forde","userId":"14193265044366032824"}},"outputId":"428b871b-eb1e-41a4-8f16-2e4fd9f0f756"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["--- QUERY: inviscid hypersonic airflows with coupled\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["  Query_ID Doc_ID  LM_Score  Rank  \\\n","0     USER    401      12.0     1   \n","1     USER    625       7.0     2   \n","2     USER   1310       7.0     3   \n","3     USER     37       7.0     4   \n","4     USER    373       7.0     5   \n","5     USER    572       6.0     6   \n","6     USER   1296       6.0     7   \n","7     USER     25       6.0     8   \n","8     USER    305       6.0     9   \n","9     USER    540       5.0    10   \n","\n","                                               Title  \n","0  inviscid hypersonic airflows with coupled non-...  \n","1      viscous and inviscid nonequilibrium gas flows  \n","2  survey of inviscid hypersonic flow theory for ...  \n","3  a new technique for investigating heat transfe...  \n","4  the generalized expansion method and its appli...  \n","5  boundary layer displacement and leading edge b...  \n","6  non-equilibrium expansions of air with coupled...  \n","7  inviscid hypersonic flow over blunt-nosed slen...  \n","8  hypersonic strong viscous interaction on a fla...  \n","9  use of local similarity concepts in hypersonic...  "],"text/html":["\n","  <div id=\"df-1bb26931-2fec-4eef-8126-23f35a4c2395\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Query_ID</th>\n","      <th>Doc_ID</th>\n","      <th>LM_Score</th>\n","      <th>Rank</th>\n","      <th>Title</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>USER</td>\n","      <td>401</td>\n","      <td>12.0</td>\n","      <td>1</td>\n","      <td>inviscid hypersonic airflows with coupled non-...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>USER</td>\n","      <td>625</td>\n","      <td>7.0</td>\n","      <td>2</td>\n","      <td>viscous and inviscid nonequilibrium gas flows</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>USER</td>\n","      <td>1310</td>\n","      <td>7.0</td>\n","      <td>3</td>\n","      <td>survey of inviscid hypersonic flow theory for ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>USER</td>\n","      <td>37</td>\n","      <td>7.0</td>\n","      <td>4</td>\n","      <td>a new technique for investigating heat transfe...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>USER</td>\n","      <td>373</td>\n","      <td>7.0</td>\n","      <td>5</td>\n","      <td>the generalized expansion method and its appli...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>USER</td>\n","      <td>572</td>\n","      <td>6.0</td>\n","      <td>6</td>\n","      <td>boundary layer displacement and leading edge b...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>USER</td>\n","      <td>1296</td>\n","      <td>6.0</td>\n","      <td>7</td>\n","      <td>non-equilibrium expansions of air with coupled...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>USER</td>\n","      <td>25</td>\n","      <td>6.0</td>\n","      <td>8</td>\n","      <td>inviscid hypersonic flow over blunt-nosed slen...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>USER</td>\n","      <td>305</td>\n","      <td>6.0</td>\n","      <td>9</td>\n","      <td>hypersonic strong viscous interaction on a fla...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>USER</td>\n","      <td>540</td>\n","      <td>5.0</td>\n","      <td>10</td>\n","      <td>use of local similarity concepts in hypersonic...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1bb26931-2fec-4eef-8126-23f35a4c2395')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1bb26931-2fec-4eef-8126-23f35a4c2395 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1bb26931-2fec-4eef-8126-23f35a4c2395');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["Display document"],"metadata":{"id":"6f0-AuMur-Lp"}},{"cell_type":"code","source":["intdocno = 401\n","\n","os.chdir(\"/content/drive/MyDrive/CA6005I - Mechanics of Search/Assignment1/Files_Individual_Docs\")\n","xml_file = \"document_\" + str(intdocno) + \".xml\"\n","\n","# parse the XML file\n","tree = ET.parse(xml_file)\n","\n","# get the root element of the XML file\n","root = tree.getroot()\n","\n","print(\"--- QUERY: \" + query + \"\\n\")\n","print(\"--- DOCUMENT: \" + \"\\n\")\n","\n","# print the contents of the XML file\n","for child in root:\n","    print(ET.tostring(child, encoding='unicode'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678983256912,"user_tz":0,"elapsed":1251,"user":{"displayName":"Anthony Forde","userId":"14193265044366032824"}},"outputId":"49e49bd2-6c6d-4672-f01f-bd421aa71726","id":"YBUdI7Vrp97Z"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["--- QUERY: inviscid hypersonic airflows with coupled\n","\n","--- DOCUMENT: \n","\n","<docno>401</docno>\n","\n","<title>inviscid hypersonic airflows with coupled non-equilibrium\n","processes .</title>\n","\n","<author>hall,j.g., eschenroeder,a.w. and marrone,p.v.</author>\n","\n","<bib>ias paper 62-67, 1962.</bib>\n","\n","<text>inviscid hypersonic airflows with coupled non-equilibrium\n","processes .\n","  analyses have been made of the effects of coupled chemical rate\n","processes in external inviscid hypersonic airflows at high enthalpy\n","levels .  exact (numerical) solutions have been obtained by the\n","inverse method for inviscid airflow over a near-spherical nose\n","under flight conditions where substantial nonequilibrium prevails\n","through the nose region .  typical conditions considered include\n","nose radii of the order of 1 ft at an altitude of 250,000 ft and\n","velocities of 15,000 and 23,000 ft per sec .\n","  the results illustrate the general importance of the coupling\n","among the reactions considered .  these included\n","dissociation-recombination, bimolecular-exchange, and ionization reactions .\n","the exact solutions show the bimolecular, no exchange reactions\n","to be important in blunt-nose flow for the kinetics of no and n,\n","as they are in the case of a plane shock wave .  an important\n","difference between blunt-nose flow and plane shock flow,\n","however, is the gasdynamic expansion in the curved shock layer of the\n","former .  this expansion reduces post-shock reaction rates .  as a\n","consequence, in the regime studied the oxygen and nitrogen-atom\n","concentrations tend to freeze in the nose region at levels below\n","those for infinite-rate equilibrium .  the reduction below the\n","equilibrium dissociation level can be large, particularly for\n","nitrogen dissociation at higher velocities .\n","  in the regime considered, the chemical kinetics are dominated\n","by two-body collision processes .  the inviscid nose flow,\n","including coupled nonequilibrium phenomena, is thus amenable to\n","binary scaling for a given velocity .  the binary scaling is\n","demonstrated for a range of altitude and scale by correlation of the\n","exact solutions for given velocity and a constant product of\n","ambient density and nose radius .  this similitude, which can\n","also scale viscous nonequilibrium and radiation phenomena in the\n","shock layer, provides a useful flexibility for hypersonic testing\n","where it is applicable .\n","  the afterbody inviscid-flow problem is briefly discussed in the\n","light of the results for the nose flow .</text>\n"]}]}]}