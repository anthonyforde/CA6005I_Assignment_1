{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPEEiibWq8/KQ2OxfoH/ZAo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Multinomial Language Model"],"metadata":{"id":"0I8ST4X3R8En"}},{"cell_type":"markdown","source":["This notebook impliments a multinomial language model for information retrieval of documents based on a query search. The documents are scored and ranked for similarity, i.e. probality of matching relevance, against a collection of queries."],"metadata":{"id":"wqxL6UOySDcX"}},{"cell_type":"markdown","source":["## Imports and setup"],"metadata":{"id":"D8L-tkAoSF-R"}},{"cell_type":"code","source":["import math\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","import csv\n","import os\n","import nltk\n","from nltk.corpus import reuters\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.text import log\n","\n","nltk.download('reuters')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","stop_words = set(stopwords.words(\"english\"))"],"metadata":{"id":"fjhGVKoI2eJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"FIvBHtii42WD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 1 - Ranking by document titles\n","In this section we score each search query for document title and create a shortlist of the top 100 relevant documents (by title)."],"metadata":{"id":"K-adKKYGpU6O"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"Kop02SIpozYJ"}},{"cell_type":"code","source":["# Create base dataframe for recording results\n","df_Results = pd.DataFrame(columns=['Query_ID','Doc_ID', 'Multinomial_Score','Query_Desc', 'Doc_Desc'])"],"metadata":{"id":"VhexWxi746vz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_Results.drop(df_Results.index,inplace=True)"],"metadata":{"id":"WeLy0YwT462z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bring in the data"],"metadata":{"id":"WOji2c3z5EST"}},{"cell_type":"markdown","source":["Indexed queries and documents preprepared from previous notebook"],"metadata":{"id":"CEAX5MqCSKpu"}},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/CA6005I - Mechanics of Search/Assignment1/Files_Indexed\")"],"metadata":{"id":"GCBWLIQq5Efa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Document titles file"],"metadata":{"id":"lsbPQdztSN6X"}},{"cell_type":"code","source":["# Import from prepared CSV file - read doc IDs and titles to array\n","with open('Indexed_Titles.csv', 'r') as file:\n","   reader = csv.reader(file)\n","   documents = []\n","   documentIDs = []\n","   for row in reader:\n","        documentIDs.append(row[1])\n","        documents.append(row[2])"],"metadata":{"id":"fARfhdXm4xOU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Search queries file"],"metadata":{"id":"BtMIXs9JSTk4"}},{"cell_type":"code","source":["# Import from prepared CSV file - read query IDs and search strings to array\n","with open('Indexed_Queries.csv', 'r') as file:\n","    reader = csv.reader(file)\n","    queries = []\n","    queryIDs = []\n","    for row in reader:\n","        queries.append(row[2])\n","        queryIDs.append((row[1]))"],"metadata":{"id":"0jmezmKj50M2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing"],"metadata":{"id":"cMNzrSXuShMX"}},{"cell_type":"code","source":["# Tokenize the documents into words\n","tokenized_docs = []\n","for doc in documents:\n","    words = doc.lower().split()\n","    words = [word for word in words if word not in stop_words]\n","    tokenized_docs.append(words)\n","\n","# Compute the vocabulary\n","vocab = set([word for doc in tokenized_docs for word in doc])\n","\n","# Compute the document-term matrix\n","doc_term_matrix = np.zeros((len(documents), len(vocab)))\n","for i, doc in enumerate(tokenized_docs):\n","    for j, word in enumerate(vocab):\n","        doc_term_matrix[i, j] = doc.count(word)"],"metadata":{"id":"LUHKo3JI5TRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocessed_docs = []\n","for doc in documents:\n","    words = doc.lower().split()\n","    words = [word for word in words if word not in stop_words]\n","    preprocessed_docs.append(words)"],"metadata":{"id":"Zn4FAUzO5OaB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Process queries and scores"],"metadata":{"id":"rGhPsOElS2iu"}},{"cell_type":"markdown","source":["For each query, a similarity score is computed for every document"],"metadata":{"id":"1gKOIXGt6uF7"}},{"cell_type":"code","source":["current_query = 0\n","# For each query\n","for query in queries:\n","\n","  doc_scores = []\n","\n","  rawquery = queries[current_query]\n","  queryID = queryIDs[current_query]\n","\n","  tokenized_query = query.lower().split()\n","  tokenized_query = [string for string in tokenized_query if string not in stop_words]  \n","\n","  # Compute the query-term vector\n","  query_term_vector = np.zeros(len(vocab))\n","  for i, word in enumerate(vocab):\n","      query_term_vector[i] = tokenized_query.count(word)\n","\n","  # Compute the document scores\n","  doc_scores = np.dot(doc_term_matrix, query_term_vector)\n","\n","  current_score = 0\n","  # For each computed similarity score\n","  for score in doc_scores:\n","    #print(\"Query: \" + str(current_query) + \" Score: \" + str(current_score) + \" \" + str(score))\n","    # Append a new row to the results dataframe\n","    new_row = [int(queryID), int(documentIDs[current_score]), score, rawquery, documents[current_score]]\n","    df_Results = df_Results.append(pd.Series(new_row, index=df_Results.columns), ignore_index=True)\n","    current_score += 1  \n","  \n","  current_query += 1"],"metadata":{"id":"7CqdBREaLIY2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sort the results: group by query ID, then sorted by scores ascending for each query. Finally, optionally, retain only top results for each query search, e.g. 10, 50, 100..."],"metadata":{"id":"lcovoVUiRc1w"}},{"cell_type":"code","source":["df_SortedResults = df_Results.sort_values(by=['Query_ID', 'Multinomial_Score'], ascending=[True, False])"],"metadata":{"id":"2FGzOIvxRVxv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Restrict to top 100 results\n","df_TopResults = df_SortedResults.groupby('Query_ID').head(100).reset_index(drop=True)"],"metadata":{"id":"WboKCIwDRV3u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults.insert(4, 'Rank',0)"],"metadata":{"id":"L8QIPCDXjSdv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults['Rank'] = df_TopResults.groupby('Query_ID').cumcount() + 1"],"metadata":{"id":"2DQ5qqOBlN1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Export final results to CSV for final analysis (outside of this notebook)\n","df_TopResults.to_csv(\"Export_Multinomial_Top100_by_Title.csv\")"],"metadata":{"id":"BNijhebvH-lx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 2 - Ranking by document contents\n","In this section we score each search query for document contents (main body of the document) and create a shortlist of the top 100 relevant documents (by contents)."],"metadata":{"id":"zofD2K6Yphw1"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"rvJxrW_Zphw2"}},{"cell_type":"code","source":["# Create base dataframe for recording results\n","df_Results = pd.DataFrame(columns=['Query_ID','Doc_ID', 'Multinomial_Score'])"],"metadata":{"id":"w8yk2Vg5phw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_Results.drop(df_Results.index,inplace=True)"],"metadata":{"id":"TUBpjft5phw2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bring in the data"],"metadata":{"id":"m-VsliFfphw2"}},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/CA6005I - Mechanics of Search/Assignment1/Files_Indexed\")"],"metadata":{"id":"345Hfyzhphw2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Indexed queries and documents preprepared from previous notebook"],"metadata":{"id":"0P2j7EIwphw3"}},{"cell_type":"markdown","source":["Document titles file"],"metadata":{"id":"rkRyE4G5phw3"}},{"cell_type":"code","source":["# Import from prepared CSV file - read doc IDs and titles to array\n","with open('Indexed_Contents.csv', 'r') as file:\n","   reader = csv.reader(file)\n","   documents = []\n","   documentIDs = []\n","   for row in reader:\n","        documentIDs.append(row[1])\n","        documents.append(row[2])"],"metadata":{"id":"K4evlRBgphw3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Search queries file"],"metadata":{"id":"ipPj5Lwwphw3"}},{"cell_type":"code","source":["# Import from prepared CSV file - read query IDs and search strings to array\n","with open('Indexed_Queries.csv', 'r') as file:\n","    reader = csv.reader(file)\n","    queries = []\n","    queryIDs = []\n","    for row in reader:\n","        queries.append(row[2])\n","        queryIDs.append((row[1]))"],"metadata":{"id":"-9ujuP4Sphw3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing"],"metadata":{"id":"dCCciOoiphw3"}},{"cell_type":"code","source":["# Tokenize the documents into words\n","tokenized_docs = []\n","for doc in documents:\n","    words = doc.lower().split()\n","    words = [word for word in words if word not in stop_words]\n","    tokenized_docs.append(words)\n","\n","# Compute the vocabulary\n","vocab = set([word for doc in tokenized_docs for word in doc])\n","\n","# Compute the document-term matrix\n","doc_term_matrix = np.zeros((len(documents), len(vocab)))\n","for i, doc in enumerate(tokenized_docs):\n","    for j, word in enumerate(vocab):\n","        doc_term_matrix[i, j] = doc.count(word)"],"metadata":{"id":"GKXoiK0yphw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocessed_docs = []\n","for doc in documents:\n","    words = doc.lower().split()\n","    words = [word for word in words if word not in stop_words]\n","    preprocessed_docs.append(words)"],"metadata":{"id":"KyIHGOLVphw4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Similarity scoring"],"metadata":{"id":"zbHXDzqYphw4"}},{"cell_type":"markdown","source":["For each query, a similarity score is computed for every document"],"metadata":{"id":"LDL0Okkiphw4"}},{"cell_type":"code","source":["current_query = 0\n","# For each query\n","for query in queries:\n","\n","  doc_scores = []\n","\n","  rawquery = queries[current_query]\n","  queryID = queryIDs[current_query]\n","\n","  tokenized_query = query.lower().split()\n","  tokenized_query = [string for string in tokenized_query if string not in stop_words]  \n","\n","  # Compute the query-term vector\n","  query_term_vector = np.zeros(len(vocab))\n","  for i, word in enumerate(vocab):\n","      query_term_vector[i] = tokenized_query.count(word)\n","\n","  # Compute the document scores\n","  doc_scores = np.dot(doc_term_matrix, query_term_vector)\n","\n","  current_score = 0\n","  # For each computed similarity score\n","  for score in doc_scores:\n","    #print(\"Query: \" + str(current_query) + \" Score: \" + str(current_score) + \" \" + str(score))\n","    # Append a new row to the results dataframe\n","    new_row = [int(queryID), int(documentIDs[current_score]), score]\n","    df_Results = df_Results.append(pd.Series(new_row, index=df_Results.columns), ignore_index=True)\n","    current_score += 1  \n","  \n","  current_query += 1"],"metadata":{"id":"a-eoXpGOphw4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sort the results: group by query ID, then sorted by scores ascending for each query. Finally, optionally, retain only top results for each query search, e.g. 10, 50, 100..."],"metadata":{"id":"8lX3VBPGphw5"}},{"cell_type":"code","source":["df_TopResults"],"metadata":{"id":"ZJ8FOioZw0fK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_SortedResults = df_Results.sort_values(by=['Query_ID', 'Multinomial_Score'], ascending=[True, False])"],"metadata":{"id":"KbI8CCzWphw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Restrict to top X results\n","df_TopResults = df_SortedResults.groupby('Query_ID').head(100).reset_index(drop=True)"],"metadata":{"id":"9XxK9U0qphw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults['Rank'] = df_TopResults.groupby('Query_ID').cumcount() + 1"],"metadata":{"id":"zF2kLmLophw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults.to_csv(\"Export_Multinomial_Top100_Queries_by_Content.csv\")"],"metadata":{"id":"RDGaZiKFphw6"},"execution_count":null,"outputs":[]}]}