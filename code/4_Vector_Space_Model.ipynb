{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bTVBv4eEy3_X6ZN9jrIq6ldi4HI6pUHa","timestamp":1677691099019},{"file_id":"1RTNSumd-g27D8N6w9ivhoTKsHeazlRrW","timestamp":1677678589981}],"authorship_tag":"ABX9TyMDcRJFZR5qlSf9jqCes1vx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","# Vector Space Model"],"metadata":{"id":"_xUxJNEVY8QH"}},{"cell_type":"markdown","source":["This notebook impliments a VSM, using Cosine Similarity, for information retrieval of documents based on a query search. The documents are scored and ranked for similarity against a collection of queries."],"metadata":{"id":"3c68PcxEq5xV"}},{"cell_type":"markdown","source":["## Imports and setup"],"metadata":{"id":"YwmxO2WVV3dk"}},{"cell_type":"code","source":["import math\n","import numpy as np\n","import pandas as pd\n","import csv\n","import os\n","import nltk\n","from nltk.corpus import reuters\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.text import log\n","\n","nltk.download('reuters')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","stop_words = set(stopwords.words(\"english\"))"],"metadata":{"id":"83vPqtruu3tj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"LQxhV4KOglC3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 1 - Ranking by document titles\n","In this section we score each search query for document title and create a shortlist of the top 100 relevant documents (by title)."],"metadata":{"id":"HllRjccnSp7l"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"8R53l2_r6eeM"}},{"cell_type":"code","source":["# Create base dataframe for recording results\n","df_Results = pd.DataFrame(columns=['Query_ID','Doc_ID', 'Cosine_Score','Query_Desc', 'Doc_Desc'])"],"metadata":{"id":"yFdEWJltdKxl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_Results.drop(df_Results.index,inplace=True)"],"metadata":{"id":"lvcA9Yuui0fb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bring in the data"],"metadata":{"id":"qGAvL3AuWFHp"}},{"cell_type":"markdown","source":["Indexed queries and documents preprepared from previous notebook"],"metadata":{"id":"4UaeZUBRWTD9"}},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/CA6005I - Mechanics of Search/Assignment1/Files_Indexed\")"],"metadata":{"id":"JLwBfx_rg_PQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Document titles file"],"metadata":{"id":"x_WU48v2Wm-v"}},{"cell_type":"code","source":["# Import from prepared CSV file - read doc IDs and titles to array\n","with open('Indexed_Titles.csv', 'r') as file:\n","    reader = csv.reader(file)\n","    documents = []\n","    documentIDs = []\n","    for row in reader:\n","        documentIDs.append(row[1])\n","        documents.append(row[2])"],"metadata":{"id":"ZHclyUE0gk47"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Search queries file"],"metadata":{"id":"sAM1IURxZjfc"}},{"cell_type":"code","source":["# Import from prepared CSV file - read query IDs and search strings to array\n","with open('Indexed_Queries.csv', 'r') as file:\n","    reader = csv.reader(file)\n","    queries = []\n","    queryIDs = []\n","    for row in reader:\n","        queries.append(row[2])\n","        queryIDs.append((row[1]))"],"metadata":{"id":"32PpkQbDZgmb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Vectorisation"],"metadata":{"id":"lRe2IjYEXpQp"}},{"cell_type":"markdown","source":["Preprocessing and stopwords removal"],"metadata":{"id":"CLuamG9dWzur"}},{"cell_type":"code","source":["# Split document titles into individual words and remove stop words\n","def preprocess(documents):\n","    #stop_words = [\"the\", \"a\", \"an\", \"and\", \"or\", \"in\", \"on\", \"at\", \"is\"]\n","    preprocessed_docs = []\n","    for doc in documents:\n","        words = doc.lower().split()\n","        words = [word for word in words if word not in stop_words]\n","        preprocessed_docs.append(words)\n","    return preprocessed_docs"],"metadata":{"id":"t8Tt-0smrBKU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocessed_docs = preprocess(documents)"],"metadata":{"id":"nKmMdCJIrBL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create vocabulary from the documents\n","vocab = sorted(set(word for doc in preprocessed_docs for word in doc))"],"metadata":{"id":"LAXk---qrBP9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vectorisation"],"metadata":{"id":"Skp4J7EQXiVI"}},{"cell_type":"code","source":["# Convert title document into a vector representation using the vocabulary\n","def vectorize(doc, vocab):\n","    vector = np.zeros(len(vocab))\n","    for word in doc:\n","        if word in vocab:\n","            vector[vocab.index(word)] += 1\n","    return vector"],"metadata":{"id":"FZL_PIuvrBRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vectorize preprocessed documents\n","vectors = [vectorize(doc, vocab) for doc in preprocessed_docs]"],"metadata":{"id":"mTfKUivDrBV5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Similarity"],"metadata":{"id":"6Pmn9bgvYNZo"}},{"cell_type":"markdown","source":["Compute cosine similarity between two vectors"],"metadata":{"id":"PiUvbwqSYXFZ"}},{"cell_type":"code","source":["def cosine_similarity(u, v):\n","    score = np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n","    if np.isnan(score):\n","      # To cater for values so close to zero they are being treated as NAN\n","      score = 0\n","    return score"],"metadata":{"id":"jrkOaJTorBXv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Process queries"],"metadata":{"id":"BteNFlyZYnPU"}},{"cell_type":"markdown","source":["For each query, a similarity score is computed for every document"],"metadata":{"id":"WFn99Yg9Z3-H"}},{"cell_type":"code","source":["current_query = 0\n","# For each query\n","for item in queries:\n","\n","  rawquery = queries[current_query]\n","  query = queries[current_query]\n","  queryID = queryIDs[current_query]\n","  query = query.split()\n","\n","  for i in range(len(query)):\n","      query[i] = query[i].lower()\n","  query = [string for string in query if string not in stop_words]  \n","  \n","  query_vec = vectorize(query, vocab)\n","\n","  # Compute cosine similarity for all documents (previously vectorised above)\n","  similarities = [cosine_similarity(query_vec, vector) for vector in vectors]\n","\n","  current_score = 0\n","  # For each computed similarity score\n","  for score in similarities:\n","    # Append a new row to the results dataframe\n","    new_row = [int(queryID), int(documentIDs[current_score]), score, rawquery, documents[current_score]]\n","    df_Results = df_Results.append(pd.Series(new_row, index=df_Results.columns), ignore_index=True)\n","    current_score += 1\n","  \n","  current_query += 1"],"metadata":{"id":"4gSB7RkP-XJC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sort the results: group by query ID, then sorted by scores ascending for each query. Finally, optionally, retain only top results for each query search, e.g. 10, 50, 100..."],"metadata":{"id":"jYrQT8ifaYr9"}},{"cell_type":"code","source":["df_SortedResults = df_Results.sort_values(by=['Query_ID', 'Cosine_Score'], ascending=[True, False])"],"metadata":{"id":"KwaW3EJgHfo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Restrict to top 100 results\n","df_TopResults = df_SortedResults.groupby('Query_ID').head(100).reset_index(drop=True)"],"metadata":{"id":"JZik2E8BHfr5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults.insert(4, 'Rank',0)"],"metadata":{"id":"L8QIPCDXjSdv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults['Rank'] = df_TopResults.groupby('Query_ID').cumcount() + 1"],"metadata":{"id":"2DQ5qqOBlN1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Export final results to CSV for final analysis (outside of this notebook)\n","df_TopResults.to_csv(\"Export_VSM_Top100_by_Title.csv\")"],"metadata":{"id":"r7DnBf_WFiy0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 2 - Ranking by document contents\n","In this section we score each search query for document contents (main body of the document) and create a shortlist of the top 100 relevant documents (by contents)."],"metadata":{"id":"L75F7jh9StVt"}},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"L3G6iCSsStVt"}},{"cell_type":"code","source":["# Create base dataframe for recording results\n","df_Results = pd.DataFrame(columns=['Query_ID','Doc_ID', 'Cosine_Score'])"],"metadata":{"id":"dGWSeLxfStVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_Results.drop(df_Results.index,inplace=True)"],"metadata":{"id":"BYxf3eGlStVu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bring in the data"],"metadata":{"id":"G4NZIND4StVu"}},{"cell_type":"markdown","source":["Indexed queries and documents preprepared from previous notebook"],"metadata":{"id":"adRKDBbQStVv"}},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/CA6005I - Mechanics of Search/Assignment1/Files_Indexed\")"],"metadata":{"id":"xxSaZApJStVv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Document titles file"],"metadata":{"id":"5GQ30t7TStVv"}},{"cell_type":"code","source":["# Import from prepared CSV file - read doc IDs and titles to array\n","with open('Indexed_Contents.csv', 'r') as file:\n","    reader = csv.reader(file)\n","    documents = []\n","    documentIDs = []\n","    for row in reader:\n","        documentIDs.append(row[1])\n","        documents.append(row[2])"],"metadata":{"id":"vObFeoSaStVw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Search queries file"],"metadata":{"id":"nB6MKJDpStVw"}},{"cell_type":"code","source":["# Import from prepared CSV file - read query IDs and search strings to array\n","with open('Indexed_Queries', 'r') as file:\n","    reader = csv.reader(file)\n","    queries = []\n","    queryIDs = []\n","    for row in reader:\n","        queries.append(row[2])\n","        queryIDs.append((row[1]))"],"metadata":{"id":"RokvLXuiStVw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Vectorisation"],"metadata":{"id":"eynUDczTStVw"}},{"cell_type":"markdown","source":["Preprocessing and stopwords removal"],"metadata":{"id":"MMQ3HxGZStVx"}},{"cell_type":"code","source":["# Split document titles into individual words and remove stop words\n","def preprocess(documents):\n","    #stop_words = [\"the\", \"a\", \"an\", \"and\", \"or\", \"in\", \"on\", \"at\", \"is\"]\n","    preprocessed_docs = []\n","    for doc in documents:\n","        words = doc.lower().split()\n","        words = [word for word in words if word not in stop_words]\n","        preprocessed_docs.append(words)\n","    return preprocessed_docs"],"metadata":{"id":"afM5IyA_StVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocessed_docs = preprocess(documents)"],"metadata":{"id":"uQ7Rc8jAStVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create vocabulary from the documents\n","vocab = sorted(set(word for doc in preprocessed_docs for word in doc))"],"metadata":{"id":"fZdspbk5StVy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vectorisation"],"metadata":{"id":"UA7D5ry4StVy"}},{"cell_type":"code","source":["# Convert title document into a vector representation using the vocabulary\n","def vectorize(doc, vocab):\n","    vector = np.zeros(len(vocab))\n","    for word in doc:\n","        if word in vocab:\n","            vector[vocab.index(word)] += 1\n","    return vector"],"metadata":{"id":"uulksT_oStVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vectorize preprocessed documents\n","vectors = [vectorize(doc, vocab) for doc in preprocessed_docs]"],"metadata":{"id":"ADnPuVDwStVz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Similarity"],"metadata":{"id":"BYdb17Q4StV0"}},{"cell_type":"markdown","source":["Compute cosine similarity between two vectors"],"metadata":{"id":"1UIA2u7cStV0"}},{"cell_type":"code","source":["def cosine_similarity(u, v):\n","    score = np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n","    if np.isnan(score):\n","      # To cater for values so close to zero they are being treated as NAN\n","      score = 0\n","    return score"],"metadata":{"id":"dR1C1JBVStV0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Process queries"],"metadata":{"id":"5okfL8sVStV1"}},{"cell_type":"markdown","source":["For each query, a similarity score is computed for every document"],"metadata":{"id":"SXgGwEIZStV1"}},{"cell_type":"code","source":["current_query = 0\n","# For each query\n","for item in queries:\n","\n","  rawquery = queries[current_query]\n","  query = queries[current_query]\n","  queryID = queryIDs[current_query]\n","  query = query.split()\n","\n","  for i in range(len(query)):\n","      query[i] = query[i].lower()\n","  query = [string for string in query if string not in stop_words]  \n","  \n","  query_vec = vectorize(query, vocab)\n","\n","  # Compute cosine similarity for all documents (previously vectorised above)\n","  similarities = [cosine_similarity(query_vec, vector) for vector in vectors]\n","\n","  current_score = 0\n","  # For each computed similarity score\n","  for score in similarities:\n","    # Append a new row to the results dataframe\n","    new_row = [int(queryID), int(documentIDs[current_score]), score]\n","    df_Results = df_Results.append(pd.Series(new_row, index=df_Results.columns), ignore_index=True)\n","    current_score += 1\n","  \n","  current_query += 1"],"metadata":{"id":"6HASBsYjStV1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sort the results: group by query ID, then sorted by scores ascending for each query. Finally, optionally, retain only top results for each query search, e.g. 10, 50, 100..."],"metadata":{"id":"zB71W_wxStV2"}},{"cell_type":"code","source":["df_SortedResults = df_Results.sort_values(by=['Query_ID', 'Cosine_Score'], ascending=[True, False])"],"metadata":{"id":"w8ZrpehgStV2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Restrict to top 100 results\n","df_TopResults = df_SortedResults.groupby('Query_ID').head(100).reset_index(drop=True)"],"metadata":{"id":"tthBZDZFStV2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults['Rank'] = df_TopResults.groupby('Query_ID').cumcount() + 1"],"metadata":{"id":"dUkogsVKStV3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_TopResults.to_csv(\"Export_VSM_Top100_by_Content.csv\")"],"metadata":{"id":"POU5dfecStV4"},"execution_count":null,"outputs":[]}]}